---
title: "TrainRearingClassifier"
author: "Lukas von Ziegler"
date: '2022-05-31'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning = FALSE}
source("DLCAnalyzer_Functions_v4.R")
library(sp)         #tested with v1.3-2
library(imputeTS)   #tested with v2.7
library(ggplot2)    #tested with v3.1.0
library(ggmap)      #tested with v3.0.0
library(data.table) #tested with v1.12.8
library(keras)      #REQUIRES TENSORFLOW INSTALL. tested with v2.2.5.0
library(readr)
library(tidyr)
library(biganalytics)
set.seed(123)
```

## Training a rearing classifier

In this script we use labeling data from a previous publication to train a classifier that can detect supported and unsupported rears.

First, we create our pipeline. This will define a unified processing method that is applied to each file

```{r}

#define pipeline
pipeline <- function(path){
  Tracking <- ReadDLCDataFromCSV(path, fps = 25)
  Tracking <- CutTrackingData(Tracking,start = 300,end = 300)
  Tracking <- CalibrateTrackingData(Tracking, "area",in.metric = 42*42, c("tr","tl","bl","br"))
  Tracking <- AddOFTZones(Tracking)
  Tracking <- CleanTrackingData(Tracking, likelihoodcutoff = 0.95, existence.pol = ScalePolygon(Tracking$zones$arena,1.3))
  Tracking <- CalculateAccelerations(Tracking)
  Tracking <- CreateSkeletonData_OFT_v4(Tracking)
  Tracking <- ZscoreNormalizeFeatures(Tracking,omit =names(Tracking$features)[c(1:23)],type = "mean")
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[1:11], factor = 4)
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[20:23], factor = 0.1)
  return(Tracking)
}
```

next, we load our labeling data (manual classifications of supported and unsupported rears of 20 different OFT files). from these we determine which files to load and specifiy the path where they are saved

```{r}
label.data <- read.table("../Rearing_Classifier/labels/AllLabDataOFT_final.csv", sep = ";", header = T)
lab <- ExtractLabels(label.data, Experimenter = "Oliver",type =c("Supported","Unsupported"))

#select files for which we have training data
files <- unique(lab$DLCFile)

#set path to CSI_SA
path <- "../data/CSI_SA/"
```

we now run the predefined pipeline on all these files and save them as 'ts'

```{r}
ts <- RunPipeline(files,path,pipeline)
```

we now add all the labeling data and create a training set with an 'integration_period' 15. More specifically this will create a training set that uses the features of a sequence of +- 15 frames to train a classifier to imitate the labeling

```{r}
#add labeling data and create training sets
for(l in names(ts)){
  ts[[l]]<- AddLabelingData(ts[[l]], lab[lab$DLCFile == ts[[l]]$filename,])
  ts[[l]] <- CreateTrainingSet(ts[[l]], integration_period = 15)
}
```

we combine all the trainings data into one big object that contains a feature matrix and the corresponding labels

```{r}
MLData <- CombineTrainingsData(ts,shuffle = TRUE)
```

we now train a neural network with a single dense layer of 1024 neurons using the training data and save it for later use.

```{r}

#initialize model
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 1024, activation = 'relu', input_shape = c(MLData$parameters$N_input),kernel_regularizer = regularizer_l2(l = 0)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = MLData$parameters$N_features, activation = 'softmax')

#define optimizer
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('categorical_accuracy')
)

#train model
history <- model %>% fit(
  MLData$train_x, MLData$train_y, 
  epochs = 30, batch_size = 512, 
  validation_split = 0
)

save_model_hdf5(model,"../Rearing_Classifier/Rearing_Classifier.rds")
saveRDS(MLData$parameters,file = "../Rearing_Classifier/Rearing_Classifier_PARAMETERS.Rds")
```