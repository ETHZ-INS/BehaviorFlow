---
title: "KmeansClustering"
author: "Lukas von Ziegler"
date: '2022-05-31'
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Kmeans Clustering of behavior

In this script we run multiple different kmeans clusterings on a subset of our data. we then train a classifier to imitate the different clustering results and apply it to all data

```{r, message=FALSE, warning = FALSE}
source("DLCAnalyzer_Functions_v4.R")
source("UnsupervisedAnalysis_Functions.R")
library(sp)         #tested with v1.3-2
library(imputeTS)   #tested with v2.7
library(ggplot2)    #tested with v3.1.0
library(ggmap)      #tested with v3.0.0
library(data.table) #tested with v1.12.8
library(cowplot)    #tested with v0.9.4
library(corrplot)   #tested with v0.84
library(keras)      #REQUIRES TENSORFLOW INSTALL. tested with v2.2.5.0
library(readr)
library(tidyr)
library(biganalytics)
library(M3C)
set.seed(123)
```

Again, first we define our pipeline

```{r, eval = FALSE}
pipeline <- function(path){
  Tracking <- ReadDLCDataFromCSV(path, fps = 25)
  Tracking$data$centre <- NULL
  Tracking <- CutTrackingData(Tracking,start = 300,end = 300)
  Tracking <- CalibrateTrackingData(Tracking, "area",in.metric = 42*42, c("tr","tl","bl","br"))
  Tracking <- AddOFTZones(Tracking)
  Tracking <- CleanTrackingData(Tracking, likelihoodcutoff = 0.95, existence.pol = ScalePolygon(Tracking$zones$arena,1.3))
  Tracking <- CalculateAccelerations(Tracking)
  Tracking <- CreateSkeletonData_OFT_v4(Tracking)
  Tracking <- ZscoreNormalizeFeatures(Tracking,omit =names(Tracking$features)[c(1:23)],type = "mean")
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[1:11], factor = 4)
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[20:23], factor = 0.1)
  Tracking <- CreateTestSet(Tracking, integration_period = 15)
  return(Tracking)
}
```

due to memory limits we do not want to run the unsupervised clustering on all 258 files. instead we select 20 files randomly from each experiment

```{r, eval = FALSE}
#number of files to include in each
n_files <- 20

#CSI data
path <- "../data/CSI_SA/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- RunPipeline(sample(files)[1:n_files],path,pipeline)

#FST data
path <- "../data/FST/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(sample(files)[1:n_files],path,pipeline))


#CRS data
path <- "../data/Tun_Yoh/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(sample(files)[1:n_files],path,pipeline))

#lets ensure we have 60 files, 20 of each experiment
names(ts)
```

we are now ready to run the unsupervised clustering. we use kmeans on the sequence data. we use 4 different selections of centers, 10, 25, 50, 100

```{r, eval = FALSE}
centers <- c(10,25,50,100)

for(i in centers){
  ts <- UnsupervisedClusteringKmeans(ts, N_clusters = i,Z_score_Normalize = TRUE)
  for(j in names(ts)){
    ts[[j]]$labels[[paste("kmeans",i,sep = ".")]] <- ts[[j]]$labels$unsupervised
    ts[[j]]$labels$unsupervised <- NULL
  }
}
```

we do the same clustering, this time after dimensionality reduction using SVD

```{r, eval = FALSE}
centers <- c(10,25,50,100)

for(i in centers){
  ts <- UnsupervisedClusteringKmeans(ts, N_clusters = i,Z_score_Normalize = TRUE,dimensions = 20)
  for(j in names(ts)){
    ts[[j]]$labels[[paste("kmeans.svd20",i,sep = ".")]] <- ts[[j]]$labels$unsupervised
    ts[[j]]$labels$unsupervised <- NULL
  }
}
```
save the data

```{r, eval = FALSE}
ts2 <- ts
for(i in names(ts2)){
  ts2[[i]]$train_x <- NULL
}
saveRDS(ts2,"../data/Original_Clusters.rds")
```

for each label we train a classifier that imitates the grouping

```{r, eval = FALSE}
ts <- readRDS("../data/Original_Clusters.rds")
for(i in names(ts[[1]]$labels[-c(1:7)])){
  print(paste("creating training set for:",i, sep = " "))
  for(j in names(ts)){
    ts[[j]] <- CreateTrainingSet(ts[[j]], integration_period = 15, label.group = i)
  }
  
  MLData <- CombineTrainingsData(ts,shuffle = TRUE)

#initialize model
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 1024, activation = 'relu', input_shape = c(MLData$parameters$N_input),kernel_regularizer = regularizer_l2(l = 0)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = MLData$parameters$N_features, activation = 'softmax')

#define optimizer
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#train model
history <- model %>% fit(
  MLData$train_x, MLData$train_y, 
  epochs = 30, batch_size = 512, 
  validation_split = 0
)

save_model_hdf5(model,paste("../kmeans_classifiers/",i,".model.hdf5", sep = ""))
saveRDS(MLData$parameters,file = paste("../kmeans_classifiers/",i,".model.PARAMETERS.rds", sep = ""))
}
```

Process all files using the neural nets for clustering

```{r, eval = FALSE}
model_classifier <- load_model_hdf5("../Rearing_Classifier/Rearing_Classifier.rds")
model_classifier_para <-  readRDS("../Rearing_Classifier/Rearing_Classifier_PARAMETERS.Rds")

model_kmeans10 <- load_model_hdf5("../kmeans_classifiers/kmeans.10.model.hdf5")
model_kmeans10_para <- readRDS("../kmeans_classifiers/kmeans.10.model.PARAMETERS.rds")
model_kmeans25 <- load_model_hdf5("../kmeans_classifiers/kmeans.25.model.hdf5")
model_kmeans25_para <- readRDS("../kmeans_classifiers/kmeans.25.model.PARAMETERS.rds")
model_kmeans50 <- load_model_hdf5("../kmeans_classifiers/kmeans.50.model.hdf5")
model_kmeans50_para <- readRDS("../kmeans_classifiers/kmeans.50.model.PARAMETERS.rds")
model_kmeans100 <- load_model_hdf5("../kmeans_classifiers/kmeans.100.model.hdf5")
model_kmeans100_para <- readRDS("../kmeans_classifiers/kmeans.100.model.PARAMETERS.rds")

model_kmeanssvd10 <- load_model_hdf5("../kmeans_classifiers/kmeans.svd20.10.model.hdf5")
model_kmeanssvd10_para <- readRDS("../kmeans_classifiers/kmeans.svd20.10.model.PARAMETERS.rds")
model_kmeanssvd25 <- load_model_hdf5("../kmeans_classifiers/kmeans.svd20.25.model.hdf5")
model_kmeanssvd25_para <- readRDS("../kmeans_classifiers/kmeans.svd20.25.model.PARAMETERS.rds")
model_kmeanssvd50 <- load_model_hdf5("../kmeans_classifiers/kmeans.svd20.50.model.hdf5")
model_kmeanssvd50_para <- readRDS("../kmeans_classifiers/kmeans.svd20.50.model.PARAMETERS.rds")
model_kmeanssvd100 <- load_model_hdf5("../kmeans_classifiers/kmeans.svd20.100.model.hdf5")
model_kmeanssvd100_para <- readRDS("../kmeans_classifiers/kmeans.svd20.100.model.PARAMETERS.rds")

pipeline <- function(path){
  Tracking <- ReadDLCDataFromCSV(path, fps = 25)
  Tracking$data$centre <- NULL
  Tracking <- CutTrackingData(Tracking,start = 300,end = 300)
  Tracking <- CalibrateTrackingData(Tracking, "area",in.metric = 42*42, c("tr","tl","bl","br"))
  Tracking <- AddOFTZones(Tracking)
  Tracking <- CleanTrackingData(Tracking, likelihoodcutoff = 0.95, existence.pol = ScalePolygon(Tracking$zones$arena,1.3))
  Tracking <- CalculateAccelerations(Tracking)
  Tracking <- CreateSkeletonData_OFT_v4(Tracking)
  Tracking <- ZscoreNormalizeFeatures(Tracking,omit =names(Tracking$features)[c(1:23)],type = "mean")
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[1:11], factor = 4)
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[20:23], factor = 0.1)
  
  Tracking <- ClassifyBehaviors(Tracking, model_classifier, model_classifier_para)
  Tracking <- OFTAnalysis(Tracking, points = "bodycentre" ,movement_cutoff = 5, integration_period = 5)
  Tracking$labels$rear.classifier <- Tracking$labels$classifications
  
  Tracking <- ClassifyBehaviors(Tracking, model_kmeans10, model_kmeans10_para)
  Tracking$labels$kmeans.10 <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_kmeans25, model_kmeans25_para)
  Tracking$labels$kmeans.25 <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_kmeans50, model_kmeans50_para)
  Tracking$labels$kmeans.50 <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_kmeans100, model_kmeans100_para)
  Tracking$labels$kmeans.100 <- Tracking$labels$classifications
  
  Tracking <- ClassifyBehaviors(Tracking, model_kmeanssvd10, model_kmeanssvd10_para)
  Tracking$labels$kmeans.svd.10 <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_kmeanssvd25, model_kmeanssvd25_para)
  Tracking$labels$kmeans.svd.25 <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_kmeanssvd50, model_kmeanssvd50_para)
  Tracking$labels$kmeans.svd.50 <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_kmeanssvd100, model_kmeanssvd100_para)
  Tracking$labels$kmeans.svd.100 <- Tracking$labels$classifications
  Tracking$labels$classifications <- NULL
  Tracking$train_x <- NULL
  return(Tracking)
}


#CSI data
path <- "../data/CSI_SA/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- RunPipeline(files,path,pipeline)

#FST data
path <- "../data/FST/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(files,path,pipeline))


#Yohimbin data
path <- "../data/Tun_Yoh/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(files,path,pipeline))

#DREADD data
path <- "../data/DREADD/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts_transfer <- RunPipeline(files,path,pipeline)

#CRS data
path <- "../data/CRS/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts_transfer <- append(ts_transfer,RunPipeline(files,path,pipeline))


for(i in names(ts)){
  ts[[i]]$train_x <- NULL
}

for(i in names(ts_transfer)){
  ts_transfer[[i]]$train_x <- NULL
}

```

Next we use the function LoadFromTrackingObject to create a unsupervised analysis dataobject. we add the metadata that contains the experimental details and save it as one object

```{r, eval = FALSE}
US <- LoadFromTrackingObject(ts)
meta <- read.table("../metadata/s2c_all.csv", sep = ";", header = T)
rownames(meta) <- meta$DLCFile
meta$DLCFile <- NULL

US <- AddMetaData(US, meta)

saveRDS(US,"../data/USData.rds")
saveRDS(ts,"../data/Trackingobject.rds")
```

We additionally save a data object that contains both original results and clustering transfer results from new experiments

```{r, eval = FALSE}
US_transfer <- LoadFromTrackingObject(append(ts,ts_transfer))
meta_transfer <- read.table("../metadata/s2c_all_transfer.csv", sep = ";", header = T)
rownames(meta_transfer) <- meta_transfer$DLCFile
meta_transfer$DLCFile <- NULL

US_transfer <- AddMetaData(US_transfer, meta_transfer)

saveRDS(US_transfer,"../data/USData_transfer.rds")
```