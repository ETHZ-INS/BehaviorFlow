---
title: "Analysis_diffClusters_MBT_LDB_IFS_Roche"
author: "Lukas von Ziegler"
date: '2024-02-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning = FALSE}
source("DLCAnalyzer.R")
source("BFA_BFF.R")
library(sp)         #tested with v1.3-2
library(imputeTS)   #tested with v2.7
library(ggplot2)    #tested with v3.1.0
library(ggmap)      #tested with v3.0.0
library(data.table) #tested with v1.12.8
library(cowplot)    #tested with v0.9.4
library(corrplot)   #tested with v0.84
library(keras)      #REQUIRES TENSORFLOW INSTALL. tested with v2.2.5.0
library(readr)
library(tidyr)
library(biganalytics)
library(M3C)
set.seed(123)
```

# Additional clustering for MBT

```{r, eval = FALSE}
pipeline <- function(path){
  Tracking <- ReadDLCDataFromCSV(path, fps = 30)
  Tracking$data$centre <- NULL
  Tracking <- CutTrackingData(Tracking,start = 300,end = 300)
  Tracking <- CalibrateTrackingData(Tracking, "area",in.metric = 27.5*37.5, c("tr","tl","bl","br"))
  Tracking <- AddOFTZones(Tracking)
  Tracking <- CleanTrackingData(Tracking, likelihoodcutoff = 0.95, existence.pol = ScalePolygon(Tracking$zones$arena,1.3))
  Tracking <- CalculateAccelerations(Tracking)
  Tracking <- CreateSkeletonData_OFT_v4(Tracking)
  Tracking <- ZscoreNormalizeFeatures(Tracking,omit =names(Tracking$features)[c(1:23)],type = "mean")
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[1:11], factor = 4)
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[20:23], factor = 0.1)
  Tracking <- CreateTestSet(Tracking, integration_period = 15)
  return(Tracking)
}
```

```{r, eval = FALSE}
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- RunPipeline(files,path,pipeline)
```

```{r, eval = FALSE}
centers <- c(10, 50, 70, 100)

for(i in centers){
  ts <- UnsupervisedClusteringKmeans(ts, N_clusters = i,Z_score_Normalize = TRUE)
  for(j in names(ts)){
    ts[[j]]$labels[[paste("kmeansMBT",i,sep = ".")]] <- ts[[j]]$labels$unsupervised
    ts[[j]]$labels$unsupervised <- NULL
  }
}
```

```{r, eval = FALSE}
ts2 <- ts
for(i in names(ts2)){
  ts2[[i]]$train_x <- NULL
}
saveRDS(ts2,"../data/TS_MBTY_diffClustering.rds")
```

```{r, eval = FALSE}
ts2 <- readRDS("../data/TS_MBTY_diffClustering.rds")
US <- LoadFromTrackingObject(ts2)
meta <- read.table("../metadata_revision/metadata_MBTY.csv", sep = ";", header = T)
rownames(meta) <- meta$DLCFile
meta$DLCFile <- NULL

US <- AddMetaData(US, meta)

saveRDS(US,"../data_revision/US_MBTY_diffClustering.rds")
```

```{r, eval = FALSE}
US <- readRDS("../data_revision/US_MBTY_diffClustering.rds")
ts_25 <- readRDS("../data/MBTY_OriginalClustering.rds")

US <- AddFromTrackingObject(US, ts_25)
US <- SmoothLabels_US(US, 5)
US <- CalculateMetrics(US)
US <- AddTransitionMatrixData(US)

saveRDS(US_comb,"../data_revision/US_MBTY_diffClustering.rds")
```

# Additional clustering for LDB

```{r, eval = FALSE}
zoneinfo <- read.table("../metadata_revision/LDB_zoneinfo.csv", sep = ";", header = T)

pipeline <- function(path){
  Tracking <- ReadDLCDataFromCSV(path, fps = 25)
  Tracking <- CleanTrackingData(Tracking, likelihoodcutoff = 0.95)
  Tracking <- CalibrateTrackingData(Tracking, method = "area",in.metric = 30*(15.7+28), points = c("tl","tr","br","bl"))
  Tracking <- AddZones(Tracking,zoneinfo)
  Tracking <- CalculateAccelerations(Tracking)
  Tracking <- CreateSkeletonData_LDB_v2(Tracking)
  Tracking <- ZscoreNormalizeFeatures(Tracking,omit = names(Tracking$features)[c(1:23)],type = "mean")
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[1:11], factor = 4)
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[20:23], factor = 0.1)
  Tracking <- LDBAnalysis(Tracking,5 ,5, points = "bodycentre")
  Tracking <- CreateTestSet(Tracking, integration_period = 15)
  return(Tracking)
}
```

```{r, eval = FALSE}
# data
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- RunPipeline(files,path,pipeline)
```

```{r, eval = FALSE}
centers <- c(10, 50, 70, 100)

for(i in centers){
  ts <- UnsupervisedClusteringKmeans(ts, N_clusters = i,Z_score_Normalize = TRUE)
  for(j in names(ts)){
    ts[[j]]$labels[[paste("kmeansLDB",i,sep = ".")]] <- ts[[j]]$labels$unsupervised
    ts[[j]]$labels$unsupervised <- NULL
  }
}
```

```{r, eval = FALSE}
ts2 <- ts
for(i in names(ts2)){
  ts2[[i]]$train_x <- NULL
}
saveRDS(ts2,"../data/TS_CRS1_LDB_diffClustering.rds")
```

```{r, eval = FALSE}
US <- LoadFromTrackingObject(ts2)
meta <- read.table("../metadata_revision/metadata_CRS1_LDB.csv", sep = ";", header = T)
rownames(meta) <- meta$DLCFile
meta$DLCFile <- NULL

US <- AddMetaData(US, meta)

saveRDS(US,"../data_revision/US_CRS1_LDB_diffClustering.rds")
```

```{r, eval = FALSE}
US <- readRDS("../data_revision/US_CRS1_LDB_diffClustering.rds")
ts_25 <- readRDS("../data/TS_CRS1_LDB.rds")

US <- AddFromTrackingObject(US, ts_25)
US <- SmoothLabels_US(US, 5)
US <- CalculateMetrics(US)
US <- AddTransitionMatrixData(US)

saveRDS(US,"../data_revision/US_CRS1_LDB_diffClustering.rds")
```

# Additional clustering for IFS shockbox

```{r, eval = FALSE}
pipeline <- function(path){
  Tracking <- ReadDLCDataFromCSV(path, fps = 25)
  Tracking$data$centre <- NULL
  Tracking <- CutTrackingData(Tracking,start = 300,end = 300)
  Tracking <- CalibrateTrackingData(Tracking, "area",in.metric = 29.5*29.5, c("tr","tl","bl","br"))
  Tracking <- AddOFTZones(Tracking)
  Tracking <- CleanTrackingData(Tracking, likelihoodcutoff = 0.95, existence.pol = ScalePolygon(Tracking$zones$arena,1.3))
  Tracking <- CalculateAccelerations(Tracking)
  Tracking <- CreateSkeletonData_OFT_v4(Tracking)
  Tracking <- ZscoreNormalizeFeatures(Tracking,omit =names(Tracking$features)[c(1:23)],type = "mean")
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[1:11], factor = 4)
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[20:23], factor = 0.1)
  Tracking <- CreateTestSet(Tracking, integration_period = 15)
  return(Tracking)
}
```

Due to memory limits we do not want to run the unsupervised clustering on all files. Instead, we select 10 files randomly from each experiment used for the "original" clustering.

```{r, eval = FALSE}
#number of files to include in each
n_files <- 10

#TFS session
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- RunPipeline(sample(files)[1:n_files],path,pipeline)

#EX1
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(sample(files)[1:n_files],path,pipeline))


#EX2
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(sample(files)[1:n_files],path,pipeline))

#EX3
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(sample(files)[1:n_files],path,pipeline))

#EX4
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(sample(files)[1:n_files],path,pipeline))

#EX5
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(sample(files)[1:n_files],path,pipeline))

#EX6
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(sample(files)[1:n_files],path,pipeline))

#lets ensure we have 70 files, 10 of each experiment
names(ts)
```

```{r, eval = FALSE}
centers <- c(10, 50, 70, 100)

for(i in centers){
  ts <- UnsupervisedClusteringKmeans(ts, N_clusters = i,Z_score_Normalize = TRUE)
  for(j in names(ts)){
    ts[[j]]$labels[[paste("kmeans",i,sep = ".")]] <- ts[[j]]$labels$unsupervised
    ts[[j]]$labels$unsupervised <- NULL
  }
}
```


```{r, eval = FALSE}
ts2 <- ts
for(i in names(ts2)){
  ts2[[i]]$train_x <- NULL
}
saveRDS(ts2,"../data/TS_IFS_Shockbox_OriginalClustering_differentClustering.rds")
```


```{r, eval = FALSE}
ts <- readRDS("../data/TS_IFS_Shockbox_OriginalClustering_differentClustering.rds")
for(i in names(ts[[1]]$labels[4])){
  print(paste("creating training set for:",i, sep = " "))
  for(j in names(ts)){
    ts[[j]] <- CreateTrainingSet(ts[[j]], integration_period = 15, label.group = i)
  }
  
  MLData <- CombineTrainingsData(ts,shuffle = TRUE)

#initialize model
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 1024, activation = 'relu', input_shape = c(MLData$parameters$N_input),kernel_regularizer = regularizer_l2(l = 0)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = MLData$parameters$N_features, activation = 'softmax')

#define optimizer
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#train model
history <- model %>% fit(
  MLData$train_x, MLData$train_y, 
  epochs = 30, batch_size = 512, 
  validation_split = 0
)

save_model_hdf5(model,paste("../kmeans_classifiers_revision/",i,"_shockbox.model.hdf5", sep = ""))
saveRDS(MLData$parameters,file = paste("../kmeans_classifiers_revision/",i,"_shockbox.model.PARAMETERS.rds", sep = ""))
}
```


```{r, eval = FALSE}

model_kmeans10 <- load_model_hdf5("../kmeans_classifiers_revision/kmeans.10_shockbox.model.hdf5")
model_kmeans10_para <- readRDS("../kmeans_classifiers_revision/kmeans.10_shockbox.model.PARAMETERS.rds")
model_kmeans50 <- load_model_hdf5("../kmeans_classifiers_revision/kmeans.50_shockbox.model.hdf5")
model_kmeans50_para <- readRDS("../kmeans_classifiers_revision/kmeans.50_shockbox.model.PARAMETERS.rds")
model_kmeans70 <- load_model_hdf5("../kmeans_classifiers_revision/kmeans.70_shockbox.model.hdf5")
model_kmeans70_para <- readRDS("../kmeans_classifiers_revision/kmeans.70_shockbox.model.PARAMETERS.rds")
model_kmeans100 <- load_model_hdf5("../kmeans_classifiers_revision/kmeans.100_shockbox.model.hdf5")
model_kmeans100_para <- readRDS("../kmeans_classifiers_revision/kmeans.100_shockbox.model.PARAMETERS.rds")



pipeline <- function(path){
  Tracking <- ReadDLCDataFromCSV(path, fps = 25)
  Tracking$data$centre <- NULL
  Tracking <- CutTrackingData(Tracking,start = 300,end = 300)
  Tracking <- CalibrateTrackingData(Tracking, "area",in.metric = 29.5*29.5, c("tr","tl","bl","br"))
  Tracking <- AddOFTZones(Tracking)
  Tracking <- CleanTrackingData(Tracking, likelihoodcutoff = 0.95, existence.pol = ScalePolygon(Tracking$zones$arena,1.3))
  Tracking <- CalculateAccelerations(Tracking)
  Tracking <- CreateSkeletonData_OFT_v4(Tracking)
  Tracking <- ZscoreNormalizeFeatures(Tracking,omit =names(Tracking$features)[c(1:23)],type = "mean")
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[1:11], factor = 4)
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[20:23], factor = 0.1)
  Tracking <- OFTAnalysis(Tracking, points = "bodycentre" ,movement_cutoff = 5, integration_period = 5)
  
  Tracking <- ClassifyBehaviors(Tracking, model_kmeans10, model_kmeans10_para)
  Tracking$labels$kmeans.10 <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_kmeans50, model_kmeans50_para)
  Tracking$labels$kmeans.50 <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_kmeans70, model_kmeans70_para)
  Tracking$labels$kmeans.70 <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_kmeans100, model_kmeans100_para)
  Tracking$labels$kmeans.100 <- Tracking$labels$classifications

  Tracking$labels$classifications <- NULL
  Tracking$train_x <- NULL
  return(Tracking)
}


#TFS session
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- RunPipeline(files,path,pipeline)

#EX1
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(files,path,pipeline))


#EX2
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(files,path,pipeline))

#EX3
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(files,path,pipeline))

#EX4
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(files,path,pipeline))

#EX5
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(files,path,pipeline))

#EX6
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(files,path,pipeline))
```

```{r, eval = FALSE}
US <- readRDS("../data_revision/US_IFS1_shockbox.rds")

US <- AddFromTrackingObject(US, ts)

saveRDS(US,"../data_revision/US_IFS1_shockbox_diffClustering.rds")
```


## Additional Clustering for Roche data

We first define our pipeline:

```{r, eval = FALSE}
pipeline <- function(path){
  Tracking <- ReadDLCDataFromCSV(path, fps = 30)
  Tracking$data$centre <- NULL
  Tracking <- CutTrackingData(Tracking,start = 300,end = 300)
  Tracking <- CalibrateTrackingData(Tracking, "area",in.metric = 40.5*40.5, c("tr","tl","bl","br"))
  Tracking <- AddOFTZones(Tracking)
  Tracking <- CleanTrackingData(Tracking, likelihoodcutoff = 0.95, existence.pol = ScalePolygon(Tracking$zones$arena,1.3))
  Tracking <- CalculateAccelerations(Tracking)
  Tracking <- CreateSkeletonData_OFT_v4(Tracking)
  Tracking <- ZscoreNormalizeFeatures(Tracking,omit =names(Tracking$features)[c(1:23)],type = "mean")
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[1:11], factor = 4)
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[20:23], factor = 0.1)
  Tracking <- CreateTestSet(Tracking, integration_period = 15)
  return(Tracking)
}
```

We then load all 20 files (10 Yohimbine, 10 Diaz):

```{r, eval = FALSE}
n_files = 10

path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- RunPipeline(sample(files)[1:n_files],path,pipeline)

path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(sample(files)[1:n_files],path,pipeline))

#Lets ensure we have all 20 files, 10 yohimbine and 10 Diaz
names(ts)
```

Run clustering with different numbers of clusters:

```{r, eval = FALSE}
centers <- c(10, 50, 70, 100)

for(i in centers){
  ts <- UnsupervisedClusteringKmeans(ts, N_clusters = i,Z_score_Normalize = TRUE)
  for(j in names(ts)){
    ts[[j]]$labels[[paste("kmeansYohDiazRoche",i,sep = ".")]] <- ts[[j]]$labels$unsupervised
    ts[[j]]$labels$unsupervised <- NULL
  }
}
```

Save the clustered data.

```{r, eval = FALSE}
ts2 <- ts
for(i in names(ts2)){
  ts2[[i]]$train_x <- NULL
}
saveRDS(ts2,"../data/TS_Roche_YohimbDiaz_OriginalClustering_differentClustering.rds")
```

Train classifiers

```{r, eval = FALSE}
ts <- readRDS("../data/TS_Roche_YohimbDiaz_OriginalClustering_differentClustering.rds")
for(i in names(ts[[1]]$labels[4])){
  print(paste("creating training set for:",i, sep = " "))
  for(j in names(ts)){
    ts[[j]] <- CreateTrainingSet(ts[[j]], integration_period = 15, label.group = i)
  }
  
  MLData <- CombineTrainingsData(ts,shuffle = TRUE)

#initialize model
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 1024, activation = 'relu', input_shape = c(MLData$parameters$N_input),kernel_regularizer = regularizer_l2(l = 0)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = MLData$parameters$N_features, activation = 'softmax')

#define optimizer
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#train model
history <- model %>% fit(
  MLData$train_x, MLData$train_y, 
  epochs = 30, batch_size = 512, 
  validation_split = 0
)

save_model_hdf5(model,paste("../kmeans_classifiers_revision/",i,".model.hdf5", sep = ""))
saveRDS(MLData$parameters,file = paste("../kmeans_classifiers_revision/",i,".model.PARAMETERS.rds", sep = ""))
}
```

Apply classifiers onto data:

```{r, eval = FALSE}
model_10 <- load_model_hdf5("../kmeans_classifiers_revision/kmeansYohDiazRoche.10.model.hdf5")
model_para_10 <- readRDS("../kmeans_classifiers_revision/kmeansYohDiazRoche.10.model.PARAMETERS.rds")
model_25 <- load_model_hdf5("../kmeans_classifiers_revision/kmeans.25_YohDiazRoche.model.hdf5")
model_para_25 <- readRDS("../kmeans_classifiers_revision/kmeans.25_YohDiazRoche.model.PARAMETERS.rds")
model_50 <- load_model_hdf5("../kmeans_classifiers_revision/kmeansYohDiazRoche.50.model.hdf5")
model_para_50 <- readRDS("../kmeans_classifiers_revision/kmeansYohDiazRoche.50.model.PARAMETERS.rds")
model_70 <- load_model_hdf5("../kmeans_classifiers_revision/kmeansYohDiazRoche.70.model.hdf5")
model_para_70 <- readRDS("../kmeans_classifiers_revision/kmeansYohDiazRoche.70.model.PARAMETERS.rds")
model_100 <- load_model_hdf5("../kmeans_classifiers_revision/kmeansYohDiazRoche.100.model.hdf5")
model_para_100 <- readRDS("../kmeans_classifiers_revision/kmeansYohDiazRoche.100.model.PARAMETERS.rds")

# model_beh <- load_model_hdf5("../NEW_kmeans_classifiers/Roche_BehaviorClassifier.model.hdf5")
# mdoel_beh_para <- readRDS("../NEW_kmeans_classifiers/Roche_BehaviorClassifier.model.PARAMETERS.rds")

pipeline <- function(path){
  Tracking <- ReadDLCDataFromCSV(path, fps = 30)
  Tracking$data$centre <- NULL
  Tracking <- CutTrackingData(Tracking,start = 300,end = 300)
  Tracking <- CalibrateTrackingData(Tracking, "area",in.metric = 40.5*40.5, c("tr","tl","bl","br"))
  Tracking <- AddOFTZones(Tracking)
  Tracking <- CleanTrackingData(Tracking, likelihoodcutoff = 0.95, existence.pol = ScalePolygon(Tracking$zones$arena,1.3))
  Tracking <- CalculateAccelerations(Tracking)
  Tracking <- CreateSkeletonData_OFT_v4(Tracking)
  Tracking <- ZscoreNormalizeFeatures(Tracking,omit =names(Tracking$features)[c(1:23)],type = "mean")
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[1:11], factor = 4)
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[20:23], factor = 0.1)
  
  # Tracking <- ClassifyBehaviors(Tracking, model_beh, mdoel_beh_para)
  # Tracking$labels$behavior <- Tracking$labels$classifications
  # Tracking$labels$classifications <- NULL
  
  Tracking <- OFTAnalysis(Tracking, points = "bodycentre" ,movement_cutoff = 5, integration_period = 5)
  
  Tracking <- ClassifyBehaviors(Tracking, model_10, model_para_10)
  Tracking$labels$kmeans.10.Roche <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_25, model_para_25)
  Tracking$labels$kmeans.25.Roche <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_50, model_para_50)
  Tracking$labels$kmeans.50.Roche <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_70, model_para_70)
  Tracking$labels$kmeans.70.Roche <- Tracking$labels$classifications
  Tracking <- ClassifyBehaviors(Tracking, model_100, model_para_100)
  Tracking$labels$kmeans.100.Roche <- Tracking$labels$classifications
  Tracking$labels$classifications <- NULL
  
  return(Tracking)
}
```

We apply it to all files

```{r, eval = FALSE}
path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- RunPipeline(files,path,pipeline)

path <- "../data/XXX/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- append(ts,RunPipeline(files,path,pipeline))

#Lets ensure we have all files
names(ts)
```

Create US Data

```{r, eval = FALSE}
US <- readRDS("../data_revision/US_YohDiazRoche_diffClustering.rds")

US <- AddFromTrackingObject(US, ts)

US <- SmoothLabels_US(US, 5)
US <- CalculateMetrics(US)
US <- AddTransitionMatrixData(US)

saveRDS(US,"../data_revision/US_YohDiazRoche_diffClustering.rds")
```