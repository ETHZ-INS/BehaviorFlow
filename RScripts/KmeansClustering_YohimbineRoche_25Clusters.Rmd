---
title: "Clustering Yohimbine Data from Roche"
author: "Fabienne RÃ¶ssler"
date: '2023-05-31'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning = FALSE}
source("DLCAnalyzer_Functions_v4.R")
source("UnsupervisedAnalysis_Functions.R")
library(sp)         #tested with v1.3-2
library(imputeTS)   #tested with v2.7
library(ggplot2)    #tested with v3.1.0
library(ggmap)      #tested with v3.0.0
library(data.table) #tested with v1.12.8
library(cowplot)    #tested with v0.9.4
library(corrplot)   #tested with v0.84
library(keras)      #REQUIRES TENSORFLOW INSTALL. tested with v2.2.5.0
library(readr)
library(tidyr)
library(biganalytics)
library(M3C)
set.seed(123)
```

# Clustering of behavior data from yohimbine experiment (performed by Roche)

## Clustering of data (k = 25)

We first define our pipeline:

```{r, eval = FALSE}
pipeline <- function(path){
  Tracking <- ReadDLCDataFromCSV(path, fps = 30)
  Tracking$data$centre <- NULL
  Tracking <- CutTrackingData(Tracking,start = 300,end = 36000)
  Tracking <- CalibrateTrackingData(Tracking, "area",in.metric = 40.5*40.5, c("tr","tl","bl","br"))
  Tracking <- AddOFTZones(Tracking)
  Tracking <- CleanTrackingData(Tracking, likelihoodcutoff = 0.95, existence.pol = ScalePolygon(Tracking$zones$arena,1.3))
  Tracking <- CalculateAccelerations(Tracking)
  Tracking <- CreateSkeletonData_OFT_v4(Tracking)
  Tracking <- ZscoreNormalizeFeatures(Tracking,omit =names(Tracking$features)[c(1:23)],type = "mean")
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[1:11], factor = 4)
  Tracking$features <- ScaleFeatures(Tracking$features, select = names(Tracking$features)[20:23], factor = 0.1)
  Tracking <- CreateTestSet(Tracking, integration_period = 15)
  return(Tracking)
}
```

We then load all 32 files:

```{r, eval = FALSE}
path <- "../data/Yohimbine_Roche/"
files <- grep(pattern = ".csv",list.files(path),value = TRUE)
ts <- RunPipeline(files,path,pipeline)

#Lets ensure we have all 32 files
names(ts)
```

We are now ready to run the k-means clustering with 25 centers.

```{r, eval = FALSE}
ts <- UnsupervisedClusteringKmeans(ts, N_clusters = 25, Z_score_Normalize = TRUE)
```

Save the clustered data.

```{r, eval = FALSE}
ts2 <- ts

for(j in names(ts2)) {
  ts2[[j]]$labels[["kmeans.25_YohRoche"]] <- ts2[[j]]$labels$unsupervised
  ts2[[j]]$labels$unsupervised <- NULL
}

for(i in names(ts2)){
  ts2[[i]]$train_x <- NULL
}

saveRDS(ts2,"../data/TS_YohimbineRoche_25Clusters.rds")
```

Create an unsupervised analysis object and add metadata.

```{r, eval = FALSE}
ts2 <- readRDS("../data/TS_YohimbineRoche_25Clusters.rds")

US <- LoadFromTrackingObject(ts2)
meta <- read.table("../metadata/Yohimbine_Roche_metadata.csv", sep = ";", header = T)
rownames(meta) <- meta$DLC.file
meta$DLC.file <- NULL

US <- AddMetaData(US, meta)

saveRDS(US,"../data/US_YohimbineRoche_25Clusters_unprocessed.rds")
```


## Preprocessing of data

We load the unsupervised analysis object.

```{r}
US <- readRDS("../data/US_YohimbineRoche_25Clusters_unprocessed.rds")
```

We first smooth the data over 5 frames, this removes clusters that were identified in a single frame and ensures that behavior trains are better resolved.

```{r, eval = FALSE}
US <- SmoothLabels_US(US, 5)
```

We calculate metrics, such as number of onset-offset and number of frames for each cluster.

```{r, eval = FALSE}
US <- CalculateMetrics(US)
```

Next, we calculate the transitionsmatrix for each clustering analysis in each file.

```{r, eval = FALSE}
US <- AddTransitionMatrixData(US)
```

We also calculate the confusion matrix between each labeling across all files. 

```{r, eval = FALSE}
US <- AddConfusionMatrix(US)
```

We save this object, so next time we do not have to re-process it.

```{r, eval = FALSE}
saveRDS(US,"../data/US_YohimbineRoche_25Clusters_processed.rds")
```


